import json
import uuid
import os
import time
from openai import OpenAI
from tqdm import tqdm


# ========= é…ç½® =========
baseurl = "https://api.deepseek.com";

client = OpenAI(api_key="", base_url=baseurl)
# model_name = "deepseek-chat"
model_name = "deepseek-chat";


REQUEST_TIMEOUT = 120
CHUNK_SIZE = 2000  # æ¯æ¬¡è¯»å–çš„æ–‡æœ¬å—å¤§å°
DELAY = 0.5        # è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰


# ========= å·¥å…·å‡½æ•° =========
def call_llm_api(prompt: str) -> str:
    """è°ƒç”¨ LLM æ¥å£å¹¶æµå¼è¾“å‡ºç»“æœ"""
    try:
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            timeout=REQUEST_TIMEOUT,
            stream=True
        )
        response_content = ""
        for chunk in response:
            if chunk.choices[0].delta.content is not None:
                response_content += chunk.choices[0].delta.content
        return response_content
    except Exception as e:
        print(f"âŒ API è°ƒç”¨å¤±è´¥: {e}")
        return ""


def semantic_splitter(text: str) -> list:
    """å°†æ–‡æœ¬æŒ‰è¯­ä¹‰å•å…ƒè¿›è¡Œåˆ‡åˆ†"""
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬åˆ†æåŠ©æ‰‹ï¼Œè¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œ**å®Œæ•´çš„è¯­ä¹‰åˆ‡åˆ†**ï¼Œå°†æ•´æ®µå†…å®¹åˆ’åˆ†ä¸ºå¤šä¸ª**è¯­ä¹‰å®Œæ•´ä¸”ç‹¬ç«‹çš„æ–‡æœ¬å—**ã€‚

#### ğŸ“Œ åˆ‡åˆ†è§„åˆ™ï¼š
1. æ¯ä¸ªè¯­ä¹‰å—åº”è¡¨è¾¾ä¸€ä¸ªç›¸å¯¹ç‹¬ç«‹çš„è¯­ä¹‰å•å…ƒï¼›
2. æ¯ä¸ªæ–‡æœ¬å—å»ºè®®é•¿åº¦åœ¨ **500 åˆ° 1000 æ±‰å­—æˆ– token** ä¹‹é—´ï¼›
3. è¯·ä¼˜å…ˆåœ¨æ ‡ç‚¹ç¬¦å·ã€æ®µè½æ¢è¡Œã€é€»è¾‘åœé¡¿ç­‰è‡ªç„¶è¯­ä¹‰è¾¹ç•Œå¤„è¿›è¡Œåˆ‡åˆ†ï¼›
4. âš ï¸ å¿…é¡»ç¡®ä¿åŸå§‹æ–‡æœ¬ä¸­çš„æ¯ä¸€å¥è¯éƒ½è¢«åŒ…å«åœ¨æŸä¸ªè¯­ä¹‰å—ä¸­ï¼Œ**ä¸å¾—çœç•¥ä»»ä½•å†…å®¹**ï¼›
5. è¾“å‡ºæ ¼å¼å¦‚ä¸‹ï¼š
   - æ¯ä¸ªè¯­ä¹‰å—ä»¥ <BLOCK_START> å¼€å¤´ï¼Œä»¥ <BLOCK_END> ç»“å°¾ï¼›
   - å—ä¸å—ä¹‹é—´ç”¨æ¢è¡Œåˆ†éš”ï¼›

#### ğŸ“„ å¾…åˆ‡åˆ†æ–‡æœ¬å¦‚ä¸‹ï¼š

{text}

#### ğŸ” è¯·åŠ¡å¿…è®°ä½ï¼š
- ä½ å¿…é¡»å®Œæˆå¯¹æ•´ä¸ªæ–‡æœ¬çš„åˆ‡åˆ†ï¼›
- ä¸å¾—æ“…è‡ªç®€åŒ–ã€æ¦‚æ‹¬æˆ–è·³è¿‡ä»»ä½•å¥å­ï¼›
- æ‰€æœ‰è¯­ä¹‰å—å¿…é¡»å¿ å®è¿˜åŸåŸæ–‡æœ¬å†…å®¹å’Œé¡ºåºã€‚
"""

    llm_output = call_llm_api(prompt)
    if not llm_output:
        return []

    blocks = []
    start_tag = "<BLOCK_START>"
    end_tag = "<BLOCK_END>"
    current_pos = 0

    while True:
        start_idx = llm_output.find(start_tag, current_pos)
        if start_idx == -1:
            break
        content_start_idx = start_idx + len(start_tag)
        end_idx = llm_output.find(end_tag, content_start_idx)
        if end_idx == -1:
            print("âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ° <BLOCK_END>ï¼Œè·³è¿‡æœªé—­åˆçš„å—ã€‚")
            break
        block_content = llm_output[content_start_idx:end_idx].strip()
        if block_content:
            blocks.append({"content": block_content, "block_id": str(uuid.uuid4())})
        current_pos = end_idx + len(end_tag)

    if not blocks and llm_output.strip():
        print("âš ï¸ LLM æœªä½¿ç”¨åˆ†éš”ç¬¦ï¼Œä½¿ç”¨å®Œæ•´è¾“å‡ºä½œä¸ºå•ä¸€å—ã€‚")
        blocks.append({"content": llm_output.strip(), "block_id": str(uuid.uuid4())})

    return blocks


def summarize_block(text: str) -> str:
    """ä¸ºæ¯ä¸ªè¯­ä¹‰å—ç”Ÿæˆæ‘˜è¦"""
    prompt = f"""
è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬å†…å®¹ç”Ÿæˆä¸€ä»½ç®€æ´ã€å‡†ç¡®çš„æ‘˜è¦ï¼Œçªå‡ºå…¶æ ¸å¿ƒè¦ç‚¹ã€‚
æ‘˜è¦é•¿åº¦åº”æ§åˆ¶åœ¨ 50 åˆ° 200 å­—ä¹‹é—´ã€‚

{text}
"""
    return call_llm_api(prompt)


def summarize_full_document(summaries: list) -> str:
    """æ ¹æ®æ‰€æœ‰å—çš„æ‘˜è¦ç”Ÿæˆå…¨æ–‡æ‘˜è¦"""
    combined_summary = "\n\n".join(summaries)
    if len(combined_summary) > 10000:
        combined_summary = combined_summary[:10000] + "...ï¼ˆæ‘˜è¦è¿‡é•¿å·²æˆªæ–­ï¼‰"

    prompt = f"""
ä»¥ä¸‹æ˜¯æ–‡æ¡£å„éƒ¨åˆ†çš„æ‘˜è¦ï¼Œè¯·æ ¹æ®å®ƒä»¬ç”Ÿæˆä¸€ä»½å®Œæ•´ã€è¿è´¯çš„å…¨æ–‡æ‘˜è¦ï¼š

{combined_summary}
"""
    return call_llm_api(prompt)


# ========= ä¸­æ–­æ¢å¤æ”¯æŒ =========
def load_resume_state(resume_file):
    if os.path.exists(resume_file):
        with open(resume_file, 'r', encoding='utf-8', errors='ignore') as f:
            return json.load(f)
    return {}


def save_resume_state(resume_file, state):
    with open(resume_file, 'w', encoding='utf-8', errors='ignore') as f:
        json.dump(state, f, ensure_ascii=False, indent=4)


# ========= ä¸»æµç¨‹å‡½æ•° =========
def process_single_file(file_info):
    input_path = file_info["input_path"]
    output_dir = file_info["output_dir"]

    resume_file = os.path.join(output_dir, "resume_state.json")
    resume_state = load_resume_state(resume_file)

    file_name = os.path.basename(input_path)
    output_blocks_json = os.path.join(output_dir, f"{file_name}_blocks.json")
    output_summary_json = os.path.join(output_dir, f"{file_name}_summary.json")
    output_blocks_temp = os.path.join(output_dir, f"{file_name}_blocks_temp.json")  # ä¸´æ—¶æ–‡ä»¶

    # åŠ è½½å·²æœ‰å—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    all_blocks = []
    if os.path.exists(output_blocks_temp):
        try:
            with open(output_blocks_temp, 'r', encoding='utf-8', errors='ignore') as f:
                all_blocks = json.load(f)
        except json.JSONDecodeError:
            print("âš ï¸ æ£€æµ‹åˆ°ä¸å®Œæ•´çš„ä¸´æ—¶æ–‡ä»¶ï¼Œå°†ä»ä¸Šæ¬¡ä½ç½®ç»§ç»­ã€‚")

    last_block = ""
    file_position = resume_state.get(file_name, 0)

    try:
        with open(input_path, 'r', encoding='utf-8', errors='ignore') as f:
            file_size = os.fstat(f.fileno()).st_size
            pbar = tqdm(total=file_size, desc=f"ğŸ“„ {file_name}", initial=file_position, unit="chars")

            while file_position < file_size:
                f.seek(file_position)
                raw_chunk = f.read(CHUNK_SIZE)
                if not raw_chunk:
                    break

                combined_text = last_block + raw_chunk
                blocks = semantic_splitter(combined_text)

                if blocks:
                    last_block = blocks[-1]["content"]
                else:
                    last_block = ""

                total_processed_length = sum(len(block["content"]) for block in blocks)

                approx_start_pos = file_position
                new_blocks = []

                for block in blocks:
                    summary = summarize_block(block["content"])
                    new_blocks.append({
                        "block_id": block["block_id"],
                        "original_text_approx_start_pos": approx_start_pos,
                        "original_text_content_snippet": block["content"][:200] + "...",
                        "summary": summary
                    })
                    approx_start_pos += len(block["content"])

                # è¿½åŠ æ–°å—åˆ°æ€»åˆ—è¡¨ï¼Œå¹¶ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                all_blocks.extend(new_blocks)
                with open(output_blocks_temp, 'w', encoding='utf-8', errors='ignore') as f_out:
                    json.dump(all_blocks, f_out, ensure_ascii=False, indent=4)

                file_position += (total_processed_length - len(last_block))
                resume_state[file_name] = file_position
                save_resume_state(resume_file, resume_state)

                pbar.update(total_processed_length)
                time.sleep(DELAY)

            pbar.close()

        # æœ€ç»ˆä¿å­˜å®Œæ•´ç»“æœ
        with open(output_blocks_json, 'w', encoding='utf-8', errors='ignore') as f:
            json.dump(all_blocks, f, ensure_ascii=False, indent=4)

        full_summary = summarize_full_document([b["summary"] for b in all_blocks])
        with open(output_summary_json, 'w', encoding='utf-8', errors='ignore') as f:
            json.dump({
                "document_title": file_name,
                "total_blocks": len(all_blocks),
                "full_document_summary": full_summary,
                "generated_date": time.strftime("%Y-%m-%d %H:%M:%S")
            }, f, ensure_ascii=False, indent=4)

        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(output_blocks_temp):
            os.remove(output_blocks_temp)

        print(f"\nâœ… å®Œæˆå¤„ç†æ–‡ä»¶: {file_name}")

    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶ {file_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
# ========= å•çº¿ç¨‹å…¥å£ =========
def batch_process_files(input_paths, output_dir="output"):
    os.makedirs(output_dir, exist_ok=True)

    file_list = [{"input_path": path, "output_dir": output_dir} for path in input_paths]

    for file_info in tqdm(file_list, desc="ğŸ§  æ€»ä½“è¿›åº¦"):
        process_single_file(file_info)


# ========= ç¤ºä¾‹å…¥å£ =========
if __name__ == "__main__":
    input_files = [
        "SemanticKB/ã€Šè¥¿æ¸¸è®°ã€‹.txt",
        # "SemanticKB/ã€Šçº¢æ¥¼æ¢¦ã€‹.txt",
        # "SemanticKB/ã€Šä¸‰å›½æ¼”ä¹‰ã€‹.txt"
    ]
    batch_process_files(input_files, output_dir="SemanticKB/output")
